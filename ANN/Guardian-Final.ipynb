import pandas as pd
import numpy as np
import joblib
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization, GlobalAveragePooling1D
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import matplotlib.pyplot as plt
# Load the data
print("Loading data...")
data = pd.read_csv('train_data.csv')
# Separate features and labels
X = data.iloc[:, :-1]
y = data.iloc[:, -1]
# Identify non-numeric columns
non_numeric_columns = X.select_dtypes(include=['object']).columns

# Debug: Display non-numeric columns
if not non_numeric_columns.empty:
    print(f"Non-numeric columns detected: {list(non_numeric_columns)}")
    print("Converting non-numeric columns to numeric values...")

# Convert non-numeric features to numeric values
label_encoders = {}
for column in non_numeric_columns:
    try:
        le = LabelEncoder()
        X[column] = le.fit_transform(X[column])
        label_encoders[column] = le
    except Exception as e:
        print(f"Error encoding column '{column}': {e}")
        raise

# Debug: Check for any remaining non-numeric values
if X.select_dtypes(include=['object']).shape[1] > 0:
    print("Warning: Non-numeric values remain. Please verify the dataset.")
    print(X.select_dtypes(include=['object']).head())
    raise ValueError("Dataset contains non-numeric values.")
# Handle missing values
print("Handling missing values...")
X = X.fillna(X.mean())
# Encode categorical features and save encoders
label_encoders = {}
for column in X.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    X[column] = le.fit_transform(X[column])
    label_encoders[column] = le
    joblib.dump(le, f'{column}_encoder.joblib')
    print(f"Encoder for column '{column}' saved as '{column}_encoder.joblib'.")

# Encode labels and save label encoder
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)
joblib.dump(label_encoder, 'label_encoder.joblib')
print("Label encoder for target saved as 'label_encoder.joblib'.")
# Remove correlated features
print("Removing highly correlated features...")
correlation_matrix = X.corr().abs()
upper_triangle = correlation_matrix.where(
    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
)
to_drop = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.9)]
X = X.drop(columns=to_drop)

# Normalize features and save scaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X)
joblib.dump(scaler, 'scaler.joblib')
print("Scaler saved as 'scaler.joblib'.")

# Stratify train-test split
print("Splitting data into training and testing sets...")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# Reshape input for CNN
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)
# Handle class imbalance with SMOTE
print("Handling class imbalance with SMOTE...")
smote = SMOTE(random_state=42)
X_train, y_train = smote.fit_resample(X_train.reshape(X_train.shape[0], -1), y_train)
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)

# Convert labels to categorical
print("Converting labels to categorical...")
num_classes = len(np.unique(y))
y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)
# Create the improved CNN model
print("Creating CNN model...")
model = Sequential([
    # Block 1
    Conv1D(64, 3, activation='relu', padding='same', input_shape=(X_train.shape[1], 1)),
    BatchNormalization(),
    Conv1D(64, 3, activation='relu', padding='same'),
    BatchNormalization(),
    Conv1D(64, 3, activation='relu', padding='same'),
    MaxPooling1D(2),
    Dropout(0.3),

    # Block 2
    Conv1D(128, 3, activation='relu', padding='same'),
    BatchNormalization(),
    Conv1D(128, 3, activation='relu', padding='same'),
    BatchNormalization(),
    Conv1D(128, 3, activation='relu', padding='same'),
    MaxPooling1D(2),
    Dropout(0.3),

    # Block 3
    Conv1D(256, 3, activation='relu', padding='same'),
    BatchNormalization(),
    Conv1D(256, 3, activation='relu', padding='same'),
    BatchNormalization(),
    Conv1D(256, 3, activation='relu', padding='same'),
    MaxPooling1D(2),
    Dropout(0.4),
    # Global pooling and dense layers
    GlobalAveragePooling1D(),
    Dense(512, activation='relu', kernel_regularizer='l2'),
    Dropout(0.5),
    Dense(256, activation='relu', kernel_regularizer='l2'),
    Dense(num_classes, activation='softmax')
])

# Print model summary
print("\nModel Summary:")
model.summary()
# Compile the model
print("Compiling model...")
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Callbacks for early stopping and learning rate reduction
callbacks = [
    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_accuracy'),
    ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)
]

# Train the model
print("Training model...")
history = model.fit(
    X_train, y_train,
    epochs=50,
    batch_size=32,
    validation_split=0.2,
    callbacks=callbacks,
    verbose=1
)
# Evaluate the model
print("Evaluating model...")
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"Test Accuracy: {test_accuracy:.4f}")
#Make predictions on test data
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test, axis=1)
# Convert predictions back to original labels
predicted_labels = label_encoder.inverse_transform(predicted_classes)
true_labels = label_encoder.inverse_transform(true_classes)

# Print sample predictions
print("\nSample predictions:")
for i in range(10):
    print(f"True: {true_labels[i]}, Predicted: {predicted_labels[i]}")
# Plot training history
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()
# Calculate and print additional metrics
from sklearn.metrics import classification_report, confusion_matrix
print("\nClassification Report:")
print(classification_report(true_labels, predicted_labels))
# Save the model
print("Saving model...")
model.save('network_traffic_model_improved.h5')
print("Model saved as 'network_traffic_model_improved.h5'")
